{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-27T23:19:13Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "544761",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.23363v1",
                "title": "MediX-R1: Open Ended Medical Reinforcement Learning",
                "updated": "2026-02-26T18:59:46Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23363v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23363v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-26T18:59:46Z",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Sahal Shaji Mullappilly"
                    },
                    {
                        "name": "Mohammed Irfan Kurpath"
                    },
                    {
                        "name": "Omair Mohamed"
                    },
                    {
                        "name": "Mohamed Zidan"
                    },
                    {
                        "name": "Fahad Khan"
                    },
                    {
                        "name": "Salman Khan"
                    },
                    {
                        "name": "Rao Anwer"
                    },
                    {
                        "name": "Hisham Cholakkal"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23360v1",
                "title": "Model Agreement via Anchoring",
                "updated": "2026-02-26T18:59:32Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23360v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23360v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Numerous lines of aim to control $\\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.\n  We develop a simple general technique for proving bounds on independent model disagreement based on $\\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:59:32Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Eric Eaton"
                    },
                    {
                        "name": "Surbhi Goel"
                    },
                    {
                        "name": "Marcel Hussing"
                    },
                    {
                        "name": "Michael Kearns"
                    },
                    {
                        "name": "Aaron Roth"
                    },
                    {
                        "name": "Sikata Bela Sengupta"
                    },
                    {
                        "name": "Jessica Sorrell"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23358v1",
                "title": "A Dataset is Worth 1 MB",
                "updated": "2026-02-26T18:59:03Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23358v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23358v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:59:03Z",
                "arxiv:comment": "23 pages, 9 figures",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Elad Kimchi Shoshani"
                    },
                    {
                        "name": "Leeyam Gabay"
                    },
                    {
                        "name": "Yedid Hoshen"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23353v1",
                "title": "SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport",
                "updated": "2026-02-26T18:55:06Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23353v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23353v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:55:06Z",
                "arxiv:comment": "Preprint",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Simon Roschmann"
                    },
                    {
                        "name": "Paul Krzakala"
                    },
                    {
                        "name": "Sonia Mazelet"
                    },
                    {
                        "name": "Quentin Bouniot"
                    },
                    {
                        "name": "Zeynep Akata"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23349v1",
                "title": "FlashOptim: Optimizers for Memory Efficient Training",
                "updated": "2026-02-26T18:52:22Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23349v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23349v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.\n  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.\n  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:52:22Z",
                "arxiv:comment": "Source code is available at https://github.com/databricks/flashoptim",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Jose Javier Gonzalez Ortiz"
                    },
                    {
                        "name": "Abhay Gupta"
                    },
                    {
                        "name": "Chris Renard"
                    },
                    {
                        "name": "Davis Blalock"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23344v1",
                "title": "Learning Contact Policies for SEIR Epidemics on Networks: A Mean-Field Game Approach",
                "updated": "2026-02-26T18:48:55Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23344v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23344v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "In this paper, we develop a mean-field game model for SEIR epidemics on heterogeneous contact networks, where individuals choose state-dependent contact effort to balance infection losses against the social and economic costs of isolation. The Nash equilibrium is characterized by a coupled Hamilton--Jacobi--Bellman/Kolmogorov system across degree classes. An important feature of the SEIR setting is the exposed compartment: the incubation period separates infection from infectiousness and changes incentives after infection occurs. In the baseline formulation, exposed agents optimally maintain full contact, while susceptible agents reduce contact according to an explicit best-response rule driven by infection pressure and the value gap. We also discuss extensions that yield nontrivial exposed precaution by introducing responsibility or compliance incentives. We establish existence of equilibrium via a fixed-point argument and prove the uniqueness under a suitable monotonicity condition. The analysis identifies a delay in the onset of precaution under longer incubation, which can lead to weaker behavioral responses and larger outbreaks. Numerical experiments illustrate how network degree and the cost exponent shape equilibrium policies and epidemic outcomes.",
                "category": {
                    "@term": "q-bio.PE",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-26T18:48:55Z",
                "arxiv:primary_category": {
                    "@term": "q-bio.PE"
                },
                "author": {
                    "name": "Weinan Wang"
                }
            },
            {
                "id": "http://arxiv.org/abs/2602.23341v1",
                "title": "Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms",
                "updated": "2026-02-26T18:47:06Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23341v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23341v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low'' information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.DS",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "math.ST",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "stat.ML",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:47:06Z",
                "arxiv:comment": "Abstract truncated to arXiv limits. To appear in ICLR'26",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Alkis Kalavasis"
                    },
                    {
                        "name": "Anay Mehrotra"
                    },
                    {
                        "name": "Manolis Zampetakis"
                    },
                    {
                        "name": "Felix Zhou"
                    },
                    {
                        "name": "Ziyu Zhu"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23339v1",
                "title": "Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?",
                "updated": "2026-02-26T18:45:33Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23339v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23339v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-26T18:45:33Z",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Tilemachos Aravanis"
                    },
                    {
                        "name": "Vladan Stojni\u0107"
                    },
                    {
                        "name": "Bill Psomas"
                    },
                    {
                        "name": "Nikos Komodakis"
                    },
                    {
                        "name": "Giorgos Tolias"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23338v1",
                "title": "CubeSounder: Low SWaP-C 180 GHz Radiometer for Atmospheric Sensing Tested on High Altitude Balloons",
                "updated": "2026-02-26T18:42:38Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23338v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23338v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Microwave sounding is the leading driver of global numerical weather forecasting, but is limited by the scalability of such instruments. With modern machining and commercial microwave components, it is now possible to design low size, weight, power, and cost (SWaP-C) microwave spectrometers while maintaining wide bandwidth performance. Here we report on the status of CubeSounder, a spectrometer tailored for water vapor radiometry that utilizes passive wave guide filter banks. After developing a prototype and high altitude balloon payload, we demonstrated CubeSounder on commercial stratospheric balloon flights. We report on our design process, especially the simulation and fabrication of the custom millimeter-wave filter banks. We also report the initial results of the data collected from the balloon flights.",
                "category": [
                    {
                        "@term": "eess.SP",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "physics.ins-det",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:42:38Z",
                "arxiv:comment": "7 Pages, 11 Figures, Submitted to IEEE Transactions on Geoscience and Remote Sensing",
                "arxiv:primary_category": {
                    "@term": "eess.SP"
                },
                "author": [
                    {
                        "name": "Kyle D. Massingill"
                    },
                    {
                        "name": "Tyler M. Karasinski"
                    },
                    {
                        "name": "Sean Bryan"
                    },
                    {
                        "name": "Michael Baricuatro"
                    },
                    {
                        "name": "Daniel Bliss"
                    },
                    {
                        "name": "Delondrae Carter"
                    },
                    {
                        "name": "Walter Goodwin"
                    },
                    {
                        "name": "Jonathan Greenfield"
                    },
                    {
                        "name": "Christopher Groppi"
                    },
                    {
                        "name": "Jae Joiner"
                    },
                    {
                        "name": "Philip Mauskopf"
                    },
                    {
                        "name": "Philip Rybak"
                    },
                    {
                        "name": "Scott Smas"
                    },
                    {
                        "name": "Roshni Suresh"
                    },
                    {
                        "name": "Joesph Tinlin"
                    },
                    {
                        "name": "Bianca Wullen"
                    },
                    {
                        "name": "Peter Wullen"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.23336v1",
                "title": "Differentiable Zero-One Loss via Hypersimplex Projections",
                "updated": "2026-02-26T18:41:31Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.23336v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.23336v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "stat.ML",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-26T18:41:31Z",
                "arxiv:comment": "To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Camilo Gomez"
                    },
                    {
                        "name": "Pengyang Wang"
                    },
                    {
                        "name": "Liansheng Tang"
                    }
                ]
            }
        ]
    }
}