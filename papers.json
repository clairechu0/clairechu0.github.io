{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-23T11:35:58Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "542991",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.18435v1",
                "title": "Assigning Confidence: K-partition Ensembles",
                "updated": "2026-02-20T18:59:53Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18435v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18435v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.",
                "category": {
                    "@term": "cs.LG",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-20T18:59:53Z",
                "arxiv:comment": "31 pages including appendix",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Aggelos Semoglou"
                    },
                    {
                        "name": "John Pavlopoulos"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18432v1",
                "title": "SARAH: Spatially Aware Real-time Agentic Humans",
                "updated": "2026-02-20T18:59:35Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18432v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18432v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user's position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-20T18:59:35Z",
                "arxiv:comment": "Project page: https://evonneng.github.io/sarah/",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Evonne Ng"
                    },
                    {
                        "name": "Siwei Zhang"
                    },
                    {
                        "name": "Zhang Chen"
                    },
                    {
                        "name": "Michael Zollhoefer"
                    },
                    {
                        "name": "Alexander Richard"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18431v1",
                "title": "SMaRT: Online Reusable Resource Assignment and an Application to Mediation in the Kenyan Judiciary",
                "updated": "2026-02-20T18:58:05Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18431v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18431v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Motivated by the problem of assigning mediators to cases in the Kenyan judicial, we study an online resource allocation problem where incoming tasks (cases) must be immediately assigned to available, capacity-constrained resources (mediators). The resources differ in their quality, which may need to be learned. In addition, resources can only be assigned to a subset of tasks that overlaps to varying degrees with the subset of tasks other resources can be assigned to. The objective is to maximize task completion while satisfying soft capacity constraints across all the resources. The scale of the real-world problem poses substantial challenges, since there are over 2000 mediators and a multitude of combinations of geographic locations (87) and case types (12) that each mediator is qualified to work on. Together, these features, unknown quality of new resources, soft capacity constraints, and a high-dimensional state space, make existing scheduling and resource allocation algorithms either inapplicable or inefficient. We formalize the problem in a tractable manner using a quadratic program formulation for assignment and a multi-agent bandit-style framework for learning. We demonstrate the key properties and advantages of our new algorithm, SMaRT (Selecting Mediators that are Right for the Task), compared with baselines on stylized instances of the mediator allocation problem. We then consider its application to real-world data on cases and mediators from the Kenyan judiciary. SMaRT outperforms baselines and allows control over the tradeoff between the strictness of capacity constraints and overall case resolution rates, both in settings where mediator quality is known beforehand and in bandit-like settings where learning is part of the problem definition. On the strength of these results, we plan to run a randomized controlled trial with SMaRT in the judiciary in the near future.",
                "category": {
                    "@term": "cs.CY",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-20T18:58:05Z",
                "arxiv:primary_category": {
                    "@term": "cs.CY"
                },
                "author": [
                    {
                        "name": "Shafkat Farabi"
                    },
                    {
                        "name": "Didac Marti Pinto"
                    },
                    {
                        "name": "Wei Lu"
                    },
                    {
                        "name": "Manuel Ramos-Maqueda"
                    },
                    {
                        "name": "Sanmay Das"
                    },
                    {
                        "name": "Antoine Deeb"
                    },
                    {
                        "name": "Anja Sautmann"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18428v1",
                "title": "The Geometry of Noise: Why Diffusion Models Don't Need Noise Conditioning",
                "updated": "2026-02-20T18:49:00Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18428v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18428v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\\text{marg}}(\\mathbf{u}) = -\\log p(\\mathbf{u})$, where $p(\\mathbf{u}) = \\int p(\\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap'' in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "eess.IV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:49:00Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Mojtaba Sahraee-Ardakan"
                    },
                    {
                        "name": "Mauricio Delbracio"
                    },
                    {
                        "name": "Peyman Milanfar"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18426v1",
                "title": "Spatio-Spectroscopic Representation Learning using Unsupervised Convolutional Long-Short Term Memory Networks",
                "updated": "2026-02-20T18:48:36Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18426v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18426v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy evolution. In this work, we demonstrate a new unsupervised deep learning framework using Convolutional Long-Short Term Memory Network Autoencoders to encode generalized feature representations across both spatial and spectroscopic dimensions spanning $19$ optical emission lines (3800A $< \u03bb<$ 8000A) among a sample of $\\sim 9000$ galaxies from the MaNGA IFS survey. As a demonstrative exercise, we assess our model on a sample of $290$ Active Galactic Nuclei (AGN) and highlight scientifically interesting characteristics of some highly anomalous AGN.",
                "category": [
                    {
                        "@term": "astro-ph.GA",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:48:36Z",
                "arxiv:comment": "This manuscript was previously submitted to ICML for peer review. Reviewers noted that while the underlying VAE-based architecture builds on established methods, its application to spatially-resolved IFS data is promising for unsupervised representation learning in astronomy. This version is released for community visibility. Reviewer decisions: Weak accept and Weak reject (Final: Reject)",
                "arxiv:primary_category": {
                    "@term": "astro-ph.GA"
                },
                "author": [
                    {
                        "name": "Kameswara Bharadwaj Mantha"
                    },
                    {
                        "name": "Lucy Fortson"
                    },
                    {
                        "name": "Ramanakumar Sankar"
                    },
                    {
                        "name": "Claudia Scarlata"
                    },
                    {
                        "name": "Chris Lintott"
                    },
                    {
                        "name": "Sandor Kruk"
                    },
                    {
                        "name": "Mike Walmsley"
                    },
                    {
                        "name": "Hugh Dickinson"
                    },
                    {
                        "name": "Karen Masters"
                    },
                    {
                        "name": "Brooke Simmons"
                    },
                    {
                        "name": "Rebecca Smethurst"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18419v1",
                "title": "Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems",
                "updated": "2026-02-20T18:41:48Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18419v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18419v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.",
                "category": [
                    {
                        "@term": "cond-mat.dis-nn",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:41:48Z",
                "arxiv:primary_category": {
                    "@term": "cond-mat.dis-nn"
                },
                "author": [
                    {
                        "name": "Geri Skenderi"
                    },
                    {
                        "name": "Lorenzo Buffoni"
                    },
                    {
                        "name": "Francesco D'Amico"
                    },
                    {
                        "name": "David Machado"
                    },
                    {
                        "name": "Raffaele Marino"
                    },
                    {
                        "name": "Matteo Negri"
                    },
                    {
                        "name": "Federico Ricci-Tersenghi"
                    },
                    {
                        "name": "Carlo Lucibello"
                    },
                    {
                        "name": "Maria Chiara Angelini"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18417v1",
                "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures",
                "updated": "2026-02-20T18:35:43Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18417v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18417v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:35:43Z",
                "arxiv:comment": "12 pages, 3 figures, 8 tables",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": {
                    "name": "Joshua Nunley"
                }
            },
            {
                "id": "http://arxiv.org/abs/2602.18415v1",
                "title": "AI-Wrapped: Participatory, Privacy-Preserving Measurement of Longitudinal LLM Use In-the-Wild",
                "updated": "2026-02-20T18:34:23Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18415v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18415v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due to privacy constraints and platform control. We present AI-Wrapped, a prototype workflow for collecting naturalistic LLM usage data while providing participants with an immediate ``wrapped''-style report on their usage statistics, top topics, and safety-relevant behavioral patterns. We report findings from an initial deployment with 82 U.S.-based adults across 48,495 conversations from their 2025 histories. Participants used LLMs for both instrumental and reflective purposes, including creative work, professional tasks, and emotional or existential themes. Some usage patterns were consistent with potential over-reliance or perfectionistic refinement, while heavier users showed comparatively more reflective exchanges than primarily transactional ones. Methodologically, even with zero data retention and PII removal, participants may remain hesitant to share chat data due to perceived privacy and judgment risks, underscoring the importance of trust, agency, and transparent design when building measurement infrastructure for alignment research.",
                "category": {
                    "@term": "cs.HC",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-20T18:34:23Z",
                "arxiv:primary_category": {
                    "@term": "cs.HC"
                },
                "author": [
                    {
                        "name": "Cathy Mengying Fang"
                    },
                    {
                        "name": "Sheer Karny"
                    },
                    {
                        "name": "Chayapatr Archiwaranguprok"
                    },
                    {
                        "name": "Yasith Samaradivakara"
                    },
                    {
                        "name": "Pat Pataranutaporn"
                    },
                    {
                        "name": "Pattie Maes"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18409v1",
                "title": "Unifying approach to uniform expressivity of graph neural networks",
                "updated": "2026-02-20T18:18:48Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18409v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18409v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:18:48Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Huan Luo"
                    },
                    {
                        "name": "Jonni Virtema"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.18406v1",
                "title": "Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges",
                "updated": "2026-02-20T18:14:05Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.18406v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.18406v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.",
                "category": [
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-20T18:14:05Z",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Minh Dinh"
                    },
                    {
                        "name": "St\u00e9phane Deny"
                    }
                ]
            }
        ]
    }
}