{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-26T12:52:26Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "544331",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.22212v1",
                "title": "Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences",
                "updated": "2026-02-25T18:59:53Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22212v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22212v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:59:53Z",
                "arxiv:comment": "CVPR 2026, Code: https://github.com/vc-bonn/neu-pig",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Julian Kaltheuner"
                    },
                    {
                        "name": "Hannah Dr\u00f6ge"
                    },
                    {
                        "name": "Markus Plack"
                    },
                    {
                        "name": "Patrick Stotko"
                    },
                    {
                        "name": "Reinhard Klein"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22209v1",
                "title": "WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos",
                "updated": "2026-02-25T18:59:10Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22209v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22209v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:59:10Z",
                "arxiv:comment": "Project website: https://judyye.github.io/whole-www",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Yufei Ye"
                    },
                    {
                        "name": "Jiaman Li"
                    },
                    {
                        "name": "Ryan Rong"
                    },
                    {
                        "name": "C. Karen Liu"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22207v1",
                "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
                "updated": "2026-02-25T18:58:25Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22207v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22207v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.",
                "category": [
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-25T18:58:25Z",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Hanna Yukhymenko"
                    },
                    {
                        "name": "Anton Alexandrov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22200v1",
                "title": "SumTablets: A Transliteration Dataset of Sumerian Tablets",
                "updated": "2026-02-25T18:50:42Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22200v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22200v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    },
                    {
                        "@rel": "related",
                        "@href": "https://doi.org/10.18653/v1/2024.ml4al-1.20",
                        "@title": "doi"
                    }
                ],
                "summary": "Sumerian transliteration is a conventional system for representing a scholar's interpretation of a tablet in the Latin script. Thanks to visionary digital Assyriology projects such as ETCSL, CDLI, and Oracc, a large number of Sumerian transliterations have been published online, and these data are well-structured for a variety of search and analysis tasks. However, the absence of a comprehensive, accessible dataset pairing transliterations with a digital representation of the tablet's cuneiform glyphs has prevented the application of modern Natural Language Processing (NLP) methods to the task of Sumerian transliteration.\n  To address this gap, we present SumTablets, a dataset pairing Unicode representations of 91,606 Sumerian cuneiform tablets (totaling 6,970,407 glyphs) with the associated transliterations published by Oracc. We construct SumTablets by first preprocessing and standardizing the Oracc transliterations before mapping each reading back to the Unicode representation of the source glyph. Further, we retain parallel structural information (e.g., surfaces, newlines, broken segments) through the use of special tokens. We release SumTablets as a Hugging Face Dataset (CC BY 4.0) and open source data preparation code via GitHub.\n  Additionally, we leverage SumTablets to implement and evaluate two transliteration baselines: (1) weighted sampling from a glyph's possible readings, and (2) fine-tuning an autoregressive language model. Our fine-tuned language model achieves an average transliteration character-level F-score (chrF) of 97.55, demonstrating the immediate potential of transformer-based transliteration models in allowing experts to rapidly verify generated transliterations rather than manually transliterating tablets one-by-one.",
                "category": {
                    "@term": "cs.CL",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:50:42Z",
                "arxiv:comment": "11 pages with 3 figures",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "arxiv:journal_ref": "Proceedings of the 1st Workshop on Machine Learning for Ancient Languages (ML4AL 2024), pages 192-202, Hybrid in Bangkok, Thailand and online. Association for Computational Linguistics",
                "author": [
                    {
                        "name": "Cole Simmons"
                    },
                    {
                        "name": "Richard Diehl Martinez"
                    },
                    {
                        "name": "Dan Jurafsky"
                    }
                ],
                "arxiv:doi": "10.18653/v1/2024.ml4al-1.20"
            },
            {
                "id": "http://arxiv.org/abs/2602.22197v1",
                "title": "Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes",
                "updated": "2026-02-25T18:46:30Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22197v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22197v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers\" using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image's utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser",
                "category": [
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-25T18:46:30Z",
                "arxiv:comment": "This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Xavier Pleimling"
                    },
                    {
                        "name": "Sifat Muhammad Abdullah"
                    },
                    {
                        "name": "Gunjan Balde"
                    },
                    {
                        "name": "Peng Gao"
                    },
                    {
                        "name": "Mainack Mondal"
                    },
                    {
                        "name": "Murtuza Jadliwala"
                    },
                    {
                        "name": "Bimal Viswanath"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22196v1",
                "title": "Reimagining Data Work: Participatory Annotation Workshops as Feminist Practice",
                "updated": "2026-02-25T18:45:42Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22196v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22196v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    },
                    {
                        "@rel": "related",
                        "@href": "https://doi.org/10.1145/3772318.3791132",
                        "@title": "doi"
                    }
                ],
                "summary": "AI systems depend on the invisible and undervalued labor of data workers, who are often treated as interchangeable units rather than collaborators with meaningful expertise. Critical scholars and practitioners have proposed alternative principles for data work, but few empirical studies examine how to enact them in practice. This paper bridges this gap through a case study of multilingual, iterative, and participatory data annotation processes with journalists and activists focused on news narratives of gender-related violence. We offer two methodological contributions. First, we demonstrate how workshops rooted in feminist epistemology can foster dialogue, build community, and disrupt knowledge hierarchies in data annotation. Second, drawing insights from practice, we deepen the analysis of existing feminist and participatory principles. We show that prioritizing context and pluralism in practice may require ``bounding'' context and working towards what we describe as a ``tactical consensus.'' We also explore tensions around materially acknowledging labor while resisting transactional researcher-participant dynamics. Through this work, we contribute to growing efforts to reimagine data and AI development as relational and political spaces for understanding difference, enacting care, and building solidarity across shared struggles.",
                "category": {
                    "@term": "cs.CY",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:45:42Z",
                "arxiv:comment": "Accepted to CHI 2026 (to appear)",
                "arxiv:primary_category": {
                    "@term": "cs.CY"
                },
                "author": [
                    {
                        "name": "Yujia Gao"
                    },
                    {
                        "name": "Isadora Araujo Crux\u00ean"
                    },
                    {
                        "name": "Helena Su\u00e1rez Val"
                    },
                    {
                        "name": "Alessandra Jungs de Almeida"
                    },
                    {
                        "name": "Catherine D'Ignazio"
                    },
                    {
                        "name": "Harini Suresh"
                    }
                ],
                "arxiv:doi": "10.1145/3772318.3791132"
            },
            {
                "id": "http://arxiv.org/abs/2602.22193v1",
                "title": "Improving Parametric Knowledge Access in Reasoning Language Models",
                "updated": "2026-02-25T18:43:01Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22193v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22193v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We study reasoning for accessing world knowledge stored in a language model's parameters. For example, recalling that Canberra is Australia's capital may benefit from thinking through major cities and the concept of purpose-built capitals. While reasoning language models are trained via reinforcement learning to produce reasoning traces on tasks such as mathematics, they may not reason well for accessing their own world knowledge. We first find that models do not generate their best world knowledge reasoning by default: adding a simple \"think step-by-step\" cue demonstrates statistically significant improvement in knowledge recall but not math. Motivated by this, we propose training models to reason over their parametric knowledge using world-knowledge question answering as a verifiable reward. After reinforcement learning on TriviaQA (+9.9%), performance also improves on Natural Questions, HotpotQA, SimpleQA, and StrategyQA by 4.2%, 2.1%, 0.6%, and 3.0%, respectively. Reasoning models are under-optimized for parametric knowledge access, but can be easily trained to reason better.",
                "category": {
                    "@term": "cs.CL",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:43:01Z",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Melody Ma"
                    },
                    {
                        "name": "John Hewitt"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22190v1",
                "title": "GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL",
                "updated": "2026-02-25T18:34:57Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22190v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22190v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-25T18:34:57Z",
                "arxiv:comment": "57 pages, 17 figures",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Qianhui Wu"
                    },
                    {
                        "name": "Zhaoyang Wang"
                    },
                    {
                        "name": "Hanyang Chen"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Hao Cheng"
                    },
                    {
                        "name": "Huaxiu Yao"
                    },
                    {
                        "name": "Baoling Peng"
                    },
                    {
                        "name": "Huan Zhang"
                    },
                    {
                        "name": "Jianfeng Gao"
                    },
                    {
                        "name": "Tong Zhang"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22188v1",
                "title": "Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach",
                "updated": "2026-02-25T18:34:03Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22188v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22188v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "physics.flu-dyn",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-25T18:34:03Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Nathalie C. Pinheiro"
                    },
                    {
                        "name": "Donghu Guo"
                    },
                    {
                        "name": "Hannah P. Menke"
                    },
                    {
                        "name": "Aniket C. Joshi"
                    },
                    {
                        "name": "Claire E. Heaney"
                    },
                    {
                        "name": "Ahmed H. ElSheikh"
                    },
                    {
                        "name": "Christopher C. Pain"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.22179v1",
                "title": "Learning and Naming Subgroups with Exceptional Survival Characteristics",
                "updated": "2026-02-25T18:25:47Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.22179v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.22179v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.",
                "category": {
                    "@term": "cs.LG",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-25T18:25:47Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Mhd Jawad Al Rahwanji"
                    },
                    {
                        "name": "Sascha Xu"
                    },
                    {
                        "name": "Nils Philipp Walter"
                    },
                    {
                        "name": "Jilles Vreeken"
                    }
                ]
            }
        ]
    }
}