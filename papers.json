{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-22T16:21:52Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "542710",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.17665v1",
                "title": "OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents",
                "updated": "2026-02-19T18:59:54Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17665v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17665v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:59:54Z",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Akashah Shabbir"
                    },
                    {
                        "name": "Muhammad Umer Sheikh"
                    },
                    {
                        "name": "Muhammad Akhtar Munir"
                    },
                    {
                        "name": "Hiyam Debary"
                    },
                    {
                        "name": "Mustansar Fiaz"
                    },
                    {
                        "name": "Muhammad Zaigham Zaheer"
                    },
                    {
                        "name": "Paolo Fraccaro"
                    },
                    {
                        "name": "Fahad Shahbaz Khan"
                    },
                    {
                        "name": "Muhammad Haris Khan"
                    },
                    {
                        "name": "Xiao Xiang Zhu"
                    },
                    {
                        "name": "Salman Khan"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17664v1",
                "title": "Sink-Aware Pruning for Diffusion Language Models",
                "updated": "2026-02-19T18:59:50Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17664v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17664v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\\bf \\texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.",
                "category": [
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-19T18:59:50Z",
                "arxiv:comment": "Code at: https://github.com/VILA-Lab/Sink-Aware-Pruning",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Aidar Myrzakhan"
                    },
                    {
                        "name": "Tianyi Li"
                    },
                    {
                        "name": "Bowei Guo"
                    },
                    {
                        "name": "Shengkun Tang"
                    },
                    {
                        "name": "Zhiqiang Shen"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17659v1",
                "title": "When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs",
                "updated": "2026-02-19T18:59:20Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17659v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17659v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $\u03c0_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.",
                "category": [
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.RO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-19T18:59:20Z",
                "arxiv:comment": "Website: https://vla-va.github.io/",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Yu Fang"
                    },
                    {
                        "name": "Yuchun Feng"
                    },
                    {
                        "name": "Dong Jing"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Yue Yang"
                    },
                    {
                        "name": "Zhenyu Wei"
                    },
                    {
                        "name": "Daniel Szafir"
                    },
                    {
                        "name": "Mingyu Ding"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17658v1",
                "title": "MARS: Margin-Aware Reward-Modeling with Self-Refinement",
                "updated": "2026-02-19T18:59:03Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17658v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17658v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model's estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.IT",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-19T18:59:03Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Payel Bhattacharjee"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    },
                    {
                        "name": "Ravi Tandon"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17655v1",
                "title": "What Language is This? Ask Your Tokenizer",
                "updated": "2026-02-19T18:58:39Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17655v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17655v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Language Identification (LID) is an important component of many multilingual natural language processing pipelines, where it facilitates corpus curation, training data analysis, and cross-lingual evaluation of large language models. Despite near-perfect performance on high-resource languages, existing systems remain brittle in low-resource and closely related language settings. We introduce UniLID, a simple and efficient LID method based on the UnigramLM tokenization algorithm, leveraging its probabilistic framing, parameter estimation technique and inference strategy. In short, we learn language-conditional unigram distributions over a shared tokenizer vocabulary but treat segmentation as a language-specific phenomenon. Our formulation is data- and compute-efficient, supports incremental addition of new languages without retraining existing models, and can naturally be integrated into existing language model tokenization pipelines. Empirical evaluations against widely used baselines, including fastText, GlotLID, and CLD3, show that UniLID achieves competitive performance on standard benchmarks, substantially improves sample efficiency in low-resource settings - surpassing 70% accuracy with as few as five labeled samples per language - and delivers large gains on fine-grained dialect identification.",
                "category": {
                    "@term": "cs.CL",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:58:39Z",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Clara Meister"
                    },
                    {
                        "name": "Ahmetcan Yavuz"
                    },
                    {
                        "name": "Pietro Lesci"
                    },
                    {
                        "name": "Tiago Pimentel"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17654v1",
                "title": "Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval",
                "updated": "2026-02-19T18:56:36Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17654v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17654v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We propose a two-stage \"Mine and Refine\" contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.",
                "category": [
                    {
                        "@term": "cs.IR",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-19T18:56:36Z",
                "arxiv:primary_category": {
                    "@term": "cs.IR"
                },
                "author": [
                    {
                        "name": "Jiaqi Xi"
                    },
                    {
                        "name": "Raghav Saboo"
                    },
                    {
                        "name": "Luming Chen"
                    },
                    {
                        "name": "Martin Wang"
                    },
                    {
                        "name": "Sudeep Das"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17653v1",
                "title": "Differences in Typological Alignment in Language Models' Treatment of Differential Argument Marking",
                "updated": "2026-02-19T18:56:34Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17653v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17653v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Recent work has shown that language models (LMs) trained on synthetic corpora can exhibit typological preferences that resemble cross-linguistic regularities in human languages, particularly for syntactic phenomena such as word order. In this paper, we extend this paradigm to differential argument marking (DAM), a semantic licensing system in which morphological marking depends on semantic prominence. Using a controlled synthetic learning method, we train GPT-2 models on 18 corpora implementing distinct DAM systems and evaluate their generalization using minimal pairs. Our results reveal a dissociation between two typological dimensions of DAM. Models reliably exhibit human-like preferences for natural markedness direction, favoring systems in which overt marking targets semantically atypical arguments. In contrast, models do not reproduce the strong object preference in human languages, in which overt marking in DAM more often targets objects rather than subjects. These findings suggest that different typological tendencies may arise from distinct underlying sources.",
                "category": {
                    "@term": "cs.CL",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:56:34Z",
                "arxiv:comment": "15 pages, 7 figures, 7 tables. Under review",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Iskar Deng"
                    },
                    {
                        "name": "Nathalia Xu"
                    },
                    {
                        "name": "Shane Steinert-Threlkeld"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17652v1",
                "title": "A Chemodynamical Census of the Milky Way's Ultra-Faint Compact Satellites. I. A First Population-Level Look at the Internal Kinematics and Metallicities of 19 Extremely-Low-Mass Halo Stellar Systems",
                "updated": "2026-02-19T18:56:28Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17652v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17652v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Deep, wide-area photometric surveys have uncovered a population of compact ($r_{1/2} \\approx$ 1-15 pc), extremely-low-mass ($M_* \\approx$ 20-4000 $M_{\\odot}$) stellar systems in the Milky Way halo that are smaller in size than known ultra-faint dwarf galaxies (UFDs) and substantially fainter than most classical globular clusters (GCs). Very little is known about the nature and origins of this population of \"Ultra-Faint Compact Satellites\" (UFCSs) owing to a dearth of spectroscopic measurements. Here, we present the first spectroscopic census of these compact systems based on Magellan/IMACS and Keck/DEIMOS observations of 19 individual UFCSs, representing $\\sim$2/3 of the known population. We securely measure mean radial velocities for all 19 systems, velocity dispersions for 15 (predominantly upper limits), metallicities for 17, metallicity dispersions for 8, and $\\textit{Gaia}$-based mean proper motions for 18. This large new spectroscopic sample provides the first insights into population-level trends for these extreme satellites. We demonstrate that: (1) the UFCSs are kinematically colder, on average, than the UFDs, disfavoring very dense dark matter halos in most cases, (2) the UFCS population is chemically diverse, spanning a factor of $\\sim$300 in mean iron abundance ($\\rm -3.3 \\lesssim [Fe/H] \\lesssim -0.8$), with multiple systems falling beneath the \"metallicity floor\" proposed for GCs, and (3) while some higher-metallicity and/or younger UFCSs are clearly star clusters, the dynamical and/or chemical evidence allows the possibility that up to $\\sim$50% of the UFCSs in our sample (9 of 19) may represent the smallest and least-massive galaxies yet discovered.",
                "category": {
                    "@term": "astro-ph.GA",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:56:28Z",
                "arxiv:comment": "63 pages (main) + 18 pages (references + appendix), 30 Figures, 6 Tables. Will submit to ApJ in one week; comments welcome. Brief summary available here: https://wcerny.github.io/compactsatellites/. Repository with spectroscopic member catalogs: https://zenodo.org/records/18612486. Forthcoming Paper II will explore the orbits, accretion histories, and tidal influences of the same sample",
                "arxiv:primary_category": {
                    "@term": "astro-ph.GA"
                },
                "author": [
                    {
                        "name": "William Cerny"
                    },
                    {
                        "name": "Ting S. Li"
                    },
                    {
                        "name": "Andrew B. Pace"
                    },
                    {
                        "name": "Joshua D. Simon"
                    },
                    {
                        "name": "Marla Geha"
                    },
                    {
                        "name": "Alexander P. Ji"
                    },
                    {
                        "name": "Alex Drlica-Wagner"
                    },
                    {
                        "name": "Jordan Bruce"
                    },
                    {
                        "name": "Oleg Y. Gnedin"
                    },
                    {
                        "name": "Eric F. Bell"
                    },
                    {
                        "name": "Sidney Mau"
                    },
                    {
                        "name": "Ivanna Escala"
                    },
                    {
                        "name": "Daisy Bissonette"
                    },
                    {
                        "name": "Alessandro Savino"
                    },
                    {
                        "name": "Anirudh Chiti"
                    },
                    {
                        "name": "Evan N. Kirby"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17650v1",
                "title": "Human-level 3D shape perception emerges from multi-view learning",
                "updated": "2026-02-19T18:56:05Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17650v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17650v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view' models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:56:05Z",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Tyler Bonnen"
                    },
                    {
                        "name": "Jitendra Malik"
                    },
                    {
                        "name": "Angjoo Kanazawa"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.17646v1",
                "title": "Multi-Round Human-AI Collaboration with User-Specified Requirements",
                "updated": "2026-02-19T18:54:34Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.17646v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.17646v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.",
                "category": {
                    "@term": "cs.LG",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-19T18:54:34Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Sima Noorani"
                    },
                    {
                        "name": "Shayan Kiyani"
                    },
                    {
                        "name": "Hamed Hassani"
                    },
                    {
                        "name": "George Pappas"
                    }
                ]
            }
        ]
    }
}