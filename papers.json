{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-17T22:24:37Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "541791",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.15030v1",
                "title": "Image Generation with a Sphere Encoder",
                "updated": "2026-02-16T18:59:57Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15030v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15030v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-16T18:59:57Z",
                "arxiv:comment": "Technical report",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Kaiyu Yue"
                    },
                    {
                        "name": "Menglin Jia"
                    },
                    {
                        "name": "Ji Hou"
                    },
                    {
                        "name": "Tom Goldstein"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15029v1",
                "title": "Symmetry in language statistics shapes the geometry of model representations",
                "updated": "2026-02-16T18:59:55Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15029v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15029v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Although learned representations underlie neural networks' success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities' latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cond-mat.dis-nn",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:59:55Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Dhruva Karkada"
                    },
                    {
                        "name": "Daniel J. Korchinski"
                    },
                    {
                        "name": "Andres Nava"
                    },
                    {
                        "name": "Matthieu Wyart"
                    },
                    {
                        "name": "Yasaman Bahri"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15028v1",
                "title": "Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization",
                "updated": "2026-02-16T18:59:42Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15028v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15028v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:59:42Z",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": {
                    "name": "Shangding Gu"
                }
            },
            {
                "id": "http://arxiv.org/abs/2602.15022v1",
                "title": "Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation",
                "updated": "2026-02-16T18:58:55Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15022v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15022v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \\times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "math.GR",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "q-bio.BM",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:58:55Z",
                "arxiv:comment": "32 pages",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Cai Zhou"
                    },
                    {
                        "name": "Zijie Chen"
                    },
                    {
                        "name": "Zian Li"
                    },
                    {
                        "name": "Jike Wang"
                    },
                    {
                        "name": "Kaiyi Jiang"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Rose Yu"
                    },
                    {
                        "name": "Muhan Zhang"
                    },
                    {
                        "name": "Stephen Bates"
                    },
                    {
                        "name": "Tommi Jaakkola"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15021v1",
                "title": "Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI",
                "updated": "2026-02-16T18:58:47Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15021v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15021v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] > -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.",
                "category": [
                    {
                        "@term": "astro-ph.SR",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "astro-ph.GA",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:58:47Z",
                "arxiv:comment": "20 pages, 13 figures, 4 tables. Submitted to AAS journals. Comments welcome",
                "arxiv:primary_category": {
                    "@term": "astro-ph.SR"
                },
                "author": [
                    {
                        "name": "Xiaosheng Zhao"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Rosemary F. G. Wyse"
                    },
                    {
                        "name": "Alexander S. Szalay"
                    },
                    {
                        "name": "Yang Huang"
                    },
                    {
                        "name": "L\u00e1szl\u00f3 Dobos"
                    },
                    {
                        "name": "Tam\u00e1s Budav\u00e1ri"
                    },
                    {
                        "name": "Viska Wei"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15019v1",
                "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation",
                "updated": "2026-02-16T18:57:49Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15019v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15019v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.\n  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",
                "category": [
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.IR",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:57:49Z",
                "arxiv:primary_category": {
                    "@term": "cs.AI"
                },
                "author": [
                    {
                        "name": "Alisa Vinogradova"
                    },
                    {
                        "name": "Vlad Vinogradov"
                    },
                    {
                        "name": "Luba Greenwood"
                    },
                    {
                        "name": "Ilya Yasny"
                    },
                    {
                        "name": "Dmitry Kobyzev"
                    },
                    {
                        "name": "Shoman Kasbekar"
                    },
                    {
                        "name": "Kong Nguyen"
                    },
                    {
                        "name": "Dmitrii Radkevich"
                    },
                    {
                        "name": "Roman Doronin"
                    },
                    {
                        "name": "Andrey Doronichev"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15018v1",
                "title": "Neurosim: A Fast Simulator for Neuromorphic Robot Perception",
                "updated": "2026-02-16T18:57:04Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15018v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15018v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .",
                "category": [
                    {
                        "@term": "cs.RO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:57:04Z",
                "arxiv:comment": "13 pages, 6 figures",
                "arxiv:primary_category": {
                    "@term": "cs.RO"
                },
                "author": [
                    {
                        "name": "Richeek Das"
                    },
                    {
                        "name": "Pratik Chaudhari"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15014v1",
                "title": "Scaling Beyond Masked Diffusion Language Models",
                "updated": "2026-02-16T18:54:47Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15014v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15014v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:54:47Z",
                "arxiv:comment": "code: https://github.com/s-sahoo/scaling-dllms",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Subham Sekhar Sahoo"
                    },
                    {
                        "name": "Jean-Marie Lemercier"
                    },
                    {
                        "name": "Zhihan Yang"
                    },
                    {
                        "name": "Justin Deschenaux"
                    },
                    {
                        "name": "Jingyu Liu"
                    },
                    {
                        "name": "John Thickstun"
                    },
                    {
                        "name": "Ante Jukic"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15012v1",
                "title": "Cold-Start Personalization via Training-Free Priors from Structured World Models",
                "updated": "2026-02-16T18:52:13Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15012v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15012v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users' stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.",
                "category": [
                    {
                        "@term": "cs.CL",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:52:13Z",
                "arxiv:comment": "24 pages, 4 figures, 4 tables",
                "arxiv:primary_category": {
                    "@term": "cs.CL"
                },
                "author": [
                    {
                        "name": "Avinandan Bose"
                    },
                    {
                        "name": "Shuyue Stella Li"
                    },
                    {
                        "name": "Faeze Brahman"
                    },
                    {
                        "name": "Pang Wei Koh"
                    },
                    {
                        "name": "Simon Shaolei Du"
                    },
                    {
                        "name": "Yulia Tsvetkov"
                    },
                    {
                        "name": "Maryam Fazel"
                    },
                    {
                        "name": "Lin Xiao"
                    },
                    {
                        "name": "Asli Celikyilmaz"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.15010v1",
                "title": "BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames",
                "updated": "2026-02-16T18:49:56Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.15010v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.15010v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.",
                "category": [
                    {
                        "@term": "cs.RO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-16T18:49:56Z",
                "arxiv:primary_category": {
                    "@term": "cs.RO"
                },
                "author": [
                    {
                        "name": "Max Sobol Mark"
                    },
                    {
                        "name": "Jacky Liang"
                    },
                    {
                        "name": "Maria Attarian"
                    },
                    {
                        "name": "Chuyuan Fu"
                    },
                    {
                        "name": "Debidatta Dwibedi"
                    },
                    {
                        "name": "Dhruv Shah"
                    },
                    {
                        "name": "Aviral Kumar"
                    }
                ]
            }
        ]
    }
}