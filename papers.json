{
    "feed": {
        "@xmlns:opensearch": "http://a9.com/-/spec/opensearch/1.1/",
        "@xmlns:arxiv": "http://arxiv.org/schemas/atom",
        "@xmlns": "http://www.w3.org/2005/Atom",
        "id": "https://arxiv.org/api/GYXgOfCBQdW2qz4J20yiNWanGQo",
        "title": "arXiv Query: search_query=all:machine OR all:learning, OR all:AI, OR all:deep OR all:learning&id_list=&start=0&max_results=10",
        "updated": "2026-02-24T12:52:28Z",
        "link": {
            "@href": "https://arxiv.org/api/query?search_query=all:machine+OR+(all:learning,+OR+(all:AI,+OR+(all:deep+OR+all:learning)))&start=0&max_results=10&id_list=",
            "@type": "application/atom+xml"
        },
        "opensearch:itemsPerPage": "10",
        "opensearch:totalResults": "543635",
        "opensearch:startIndex": "0",
        "entry": [
            {
                "id": "http://arxiv.org/abs/2602.20160v1",
                "title": "tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction",
                "updated": "2026-02-23T18:59:45Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20160v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20160v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model's capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-23T18:59:45Z",
                "arxiv:comment": "Accepted by CVPR 2026. Project Page: https://cwchenwang.github.io/tttLRM",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Wang Yifan"
                    },
                    {
                        "name": "Zhiqin Chen"
                    },
                    {
                        "name": "Yuheng Liu"
                    },
                    {
                        "name": "Kalyan Sunkavalli"
                    },
                    {
                        "name": "Sai Bi"
                    },
                    {
                        "name": "Lingjie Liu"
                    },
                    {
                        "name": "Yiwei Hu"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20159v1",
                "title": "A Very Big Video Reasoning Suite",
                "updated": "2026-02-23T18:59:41Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20159v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20159v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .",
                "category": [
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.MM",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.RO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:59:41Z",
                "arxiv:comment": "Homepage: https://video-reason.com/",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Maijunxian Wang"
                    },
                    {
                        "name": "Ruisi Wang"
                    },
                    {
                        "name": "Juyi Lin"
                    },
                    {
                        "name": "Ran Ji"
                    },
                    {
                        "name": "Thadd\u00e4us Wiedemer"
                    },
                    {
                        "name": "Qingying Gao"
                    },
                    {
                        "name": "Dezhi Luo"
                    },
                    {
                        "name": "Yaoyao Qian"
                    },
                    {
                        "name": "Lianyu Huang"
                    },
                    {
                        "name": "Zelong Hong"
                    },
                    {
                        "name": "Jiahui Ge"
                    },
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Hang He"
                    },
                    {
                        "name": "Yifan Zhou"
                    },
                    {
                        "name": "Lingzi Guo"
                    },
                    {
                        "name": "Lantao Mei"
                    },
                    {
                        "name": "Jiachen Li"
                    },
                    {
                        "name": "Hanwen Xing"
                    },
                    {
                        "name": "Tianqi Zhao"
                    },
                    {
                        "name": "Fengyuan Yu"
                    },
                    {
                        "name": "Weihang Xiao"
                    },
                    {
                        "name": "Yizheng Jiao"
                    },
                    {
                        "name": "Jianheng Hou"
                    },
                    {
                        "name": "Danyang Zhang"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Boyang Zhong"
                    },
                    {
                        "name": "Zehong Zhao"
                    },
                    {
                        "name": "Gaoyun Fang"
                    },
                    {
                        "name": "John Kitaoka"
                    },
                    {
                        "name": "Yile Xu"
                    },
                    {
                        "name": "Hua Xu"
                    },
                    {
                        "name": "Kenton Blacutt"
                    },
                    {
                        "name": "Tin Nguyen"
                    },
                    {
                        "name": "Siyuan Song"
                    },
                    {
                        "name": "Haoran Sun"
                    },
                    {
                        "name": "Shaoyue Wen"
                    },
                    {
                        "name": "Linyang He"
                    },
                    {
                        "name": "Runming Wang"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Mengyue Yang"
                    },
                    {
                        "name": "Ziqiao Ma"
                    },
                    {
                        "name": "Rapha\u00ebl Milli\u00e8re"
                    },
                    {
                        "name": "Freda Shi"
                    },
                    {
                        "name": "Nuno Vasconcelos"
                    },
                    {
                        "name": "Daniel Khashabi"
                    },
                    {
                        "name": "Alan Yuille"
                    },
                    {
                        "name": "Yilun Du"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Dahua Lin"
                    },
                    {
                        "name": "Ziwei Liu"
                    },
                    {
                        "name": "Vikash Kumar"
                    },
                    {
                        "name": "Yijiang Li"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Zhongang Cai"
                    },
                    {
                        "name": "Hokin Deng"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20157v1",
                "title": "Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning",
                "updated": "2026-02-23T18:59:30Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20157v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20157v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow') as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.",
                "category": {
                    "@term": "cs.CV",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-23T18:59:30Z",
                "arxiv:comment": "CVPR 2026. Project website: https://flow3r-project.github.io/",
                "arxiv:primary_category": {
                    "@term": "cs.CV"
                },
                "author": [
                    {
                        "name": "Zhongxiao Cong"
                    },
                    {
                        "name": "Qitao Zhao"
                    },
                    {
                        "name": "Minsik Jeon"
                    },
                    {
                        "name": "Shubham Tulsiani"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20156v1",
                "title": "Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks",
                "updated": "2026-02-23T18:59:27Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20156v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20156v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today's agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.",
                "category": [
                    {
                        "@term": "cs.CR",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:59:27Z",
                "arxiv:primary_category": {
                    "@term": "cs.CR"
                },
                "author": [
                    {
                        "name": "David Schmotz"
                    },
                    {
                        "name": "Luca Beurer-Kellner"
                    },
                    {
                        "name": "Sahar Abdelnabi"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20153v1",
                "title": "JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks",
                "updated": "2026-02-23T18:59:10Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20153v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20153v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models' internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.",
                "category": [
                    {
                        "@term": "stat.ML",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "stat.ME",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:59:10Z",
                "arxiv:comment": "11 pages + appendix. Preliminary version of an ongoing project that will be expanded with furhter evaluations",
                "arxiv:primary_category": {
                    "@term": "stat.ML"
                },
                "author": [
                    {
                        "name": "Jakob Heiss"
                    },
                    {
                        "name": "S\u00f6ren Lambrecht"
                    },
                    {
                        "name": "Jakob Weissteiner"
                    },
                    {
                        "name": "Hanna Wutte"
                    },
                    {
                        "name": "\u017dan \u017duri\u010d"
                    },
                    {
                        "name": "Josef Teichmann"
                    },
                    {
                        "name": "Bin Yu"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20152v1",
                "title": "Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data",
                "updated": "2026-02-23T18:59:04Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20152v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20152v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.",
                "category": [
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "stat.ML",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:59:04Z",
                "arxiv:comment": "ICLR 2026",
                "arxiv:primary_category": {
                    "@term": "cs.LG"
                },
                "author": [
                    {
                        "name": "Zhenyao Ma"
                    },
                    {
                        "name": "Yue Liang"
                    },
                    {
                        "name": "Dongxu Li"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20151v1",
                "title": "Conformal Risk Control for Non-Monotonic Losses",
                "updated": "2026-02-23T18:58:54Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20151v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20151v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.",
                "category": [
                    {
                        "@term": "stat.ME",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.LG",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "math.ST",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "stat.ML",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:58:54Z",
                "arxiv:primary_category": {
                    "@term": "stat.ME"
                },
                "author": {
                    "name": "Anastasios N. Angelopoulos"
                }
            },
            {
                "id": "http://arxiv.org/abs/2602.20150v1",
                "title": "Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization",
                "updated": "2026-02-23T18:58:24Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20150v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20150v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.",
                "category": [
                    {
                        "@term": "cs.RO",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.CV",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:58:24Z",
                "arxiv:comment": "15 pages, 13 figures, in submission",
                "arxiv:primary_category": {
                    "@term": "cs.RO"
                },
                "author": [
                    {
                        "name": "Wei-Cheng Huang"
                    },
                    {
                        "name": "Jiaheng Han"
                    },
                    {
                        "name": "Xiaohan Ye"
                    },
                    {
                        "name": "Zherong Pan"
                    },
                    {
                        "name": "Kris Hauser"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20144v1",
                "title": "Agentic AI for Scalable and Robust Optical Systems Control",
                "updated": "2026-02-23T18:54:32Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20144v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20144v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.",
                "category": [
                    {
                        "@term": "eess.SY",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.AI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    },
                    {
                        "@term": "cs.NI",
                        "@scheme": "http://arxiv.org/schemas/atom"
                    }
                ],
                "published": "2026-02-23T18:54:32Z",
                "arxiv:primary_category": {
                    "@term": "eess.SY"
                },
                "author": [
                    {
                        "name": "Zehao Wang"
                    },
                    {
                        "name": "Mingzhe Han"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Yue-Kai Huang"
                    },
                    {
                        "name": "Philip Ji"
                    },
                    {
                        "name": "Denton Wu"
                    },
                    {
                        "name": "Mahdi Safari"
                    },
                    {
                        "name": "Flemming Holtorf"
                    },
                    {
                        "name": "Kenaish AlQubaisi"
                    },
                    {
                        "name": "Norbert M. Linke"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    },
                    {
                        "name": "Ting Wang"
                    },
                    {
                        "name": "Dirk Englund"
                    },
                    {
                        "name": "Tingjun Chen"
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2602.20140v1",
                "title": "PackFlow: Generative Molecular Crystal Structure Prediction via Reinforcement Learning Alignment",
                "updated": "2026-02-23T18:52:13Z",
                "link": [
                    {
                        "@href": "https://arxiv.org/abs/2602.20140v1",
                        "@rel": "alternate",
                        "@type": "text/html"
                    },
                    {
                        "@href": "https://arxiv.org/pdf/2602.20140v1",
                        "@rel": "related",
                        "@type": "application/pdf",
                        "@title": "pdf"
                    }
                ],
                "summary": "Organic molecular crystals underpin technologies ranging from pharmaceuticals to organic electronics, yet predicting solid-state packing of molecules remains challenging because candidate generation is combinatorial and stability is only resolved after costly energy evaluations. Here we introduce PackFlow, a flow matching framework for molecular crystal structure prediction (CSP) that generates heavy-atom crystal proposals by jointly sampling Cartesian coordinates and unit-cell lattice parameters given a molecular graph. This lattice-aware generation interfaces directly with downstream relaxation and lattice-energy ranking, positioning PackFlow as a scalable proposal engine within standard CSP pipelines. To explicitly steer generation toward physically favourable regions, we propose physics alignment, a reinforcement learning post-training stage that uses machine-learned interatomic potential energies and forces as stability proxies. Physics alignment improves physical validity without altering inference-time sampling. We validate PackFlow's performance against heuristic baselines through two distinct evaluations. First, on a broad unseen set of molecular systems, we demonstrate superior candidate generation capability, with proposals exhibiting greater structural similarity to experimental polymorphs. Second, we assess the full end-to-end workflow on two unseen CSP blind-test case studies, including relaxation and lattice-energy analysis. In both settings, PackFlow outperforms heuristics-based methods by concentrating probability mass in low-energy basins, yielding candidates that relax into lower-energy minima and offering a practical route to amortize the relax-and-rank bottleneck.",
                "category": {
                    "@term": "physics.chem-ph",
                    "@scheme": "http://arxiv.org/schemas/atom"
                },
                "published": "2026-02-23T18:52:13Z",
                "arxiv:primary_category": {
                    "@term": "physics.chem-ph"
                },
                "author": [
                    {
                        "name": "Akshay Subramanian"
                    },
                    {
                        "name": "Elton Pan"
                    },
                    {
                        "name": "Juno Nam"
                    },
                    {
                        "name": "Maurice Weiler"
                    },
                    {
                        "name": "Shuhui Qu"
                    },
                    {
                        "name": "Cheol Woo Park"
                    },
                    {
                        "name": "Tommi S. Jaakkola"
                    },
                    {
                        "name": "Elsa Olivetti"
                    },
                    {
                        "name": "Rafael Gomez-Bombarelli"
                    }
                ]
            }
        ]
    }
}